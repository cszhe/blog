Title: 到底谁在吃垃圾
Date: 2025-09-17 17:06
Category:  工作和学习
tags: 人工智能, LLM

## 前言

之前弄了个小玩具, 用LLM来响应网页的请求. 因为代码很短, 直接就贴在这里了.

```python
#!/usr/bin/env python3
# from google import genai
# from google.genai import types
from cerebras.cloud.sdk import Cerebras
import os
import logging
from logging.handlers import RotatingFileHandler
from flask import Flask, request
from pathvalidate import sanitize_filename
import dotenv
dotenv.load_dotenv()

# The client gets the API key from the environment variable `GEMINI_API_KEY`.
# client = genai.Client()

client = Cerebras(
  api_key=os.environ.get("CEREBRAS_API_KEY"),
)

app = Flask(__name__)

os.makedirs("html", exist_ok=True)
os.makedirs("logs", exist_ok=True)

# Configure logging: console + rotating file
logger = logging.getLogger("ai_server")
logger.setLevel(logging.INFO)
formatter = logging.Formatter("%(asctime)s %(levelname)s %(message)s")

console_handler = logging.StreamHandler()
console_handler.setFormatter(formatter)
logger.addHandler(console_handler)

file_handler = RotatingFileHandler("logs/requests.log", maxBytes=5 * 1024 * 1024, backupCount=5)
file_handler.setFormatter(formatter)
logger.addHandler(file_handler)

@app.route('/', defaults={'path': ''}, methods=['GET', 'POST'])
@app.route('/<path:path>', methods=['GET', 'POST'])
def catch_all(path):
    # Log incoming request
    req_path = request.full_path.strip("?")
    client_ip = request.remote_addr or "-"
    logger.info("Incoming request: method=%s path=%s client=%s UA=%s", request.method, req_path, client_ip, request.headers.get("User-Agent"))

    path = req_path
    filename = f"html/" + sanitize_filename(path) + ".html"
    if os.path.isfile(filename):
        with open(filename, "r") as f:
            content = f.read()
            # logger.info("Serving cached file=%s size=%d", filename, len(content))
            return content

    try:
        # Build and send prompt to the model
        # prompt_context = f"You are a web server. A request has just come in on the path {path}. Respond with HTML/CSS/JS that is appropriate for that path. Only return the HTML/CSS/JS, nothing else. Do not include any comments or explanations. Try to make the web page as beautiful and functional as possible. Don't mention that this is a simulation, make it as realistic as possible. You can add relative links if appropriate, but if the user follows a relative link, you'll have to handle that too, so be sure to pass enough context for yourself to understand what you're doing in the URL."
        # logger.info("Sending prompt to model (truncated): %s", prompt_context[:200])

        result = client.chat.completions.create(
            messages=[
                {"role": "user", "content": f"""You are a web server. A request has just come in on the path {path}.
                    Respond with HTML/CSS/JS that is appropriate for that path.
                    Only return the HTML/CSS/JS, nothing else.
                    Do not include any comments or explanations.
                    Try to make the web page as beautiful and functional as possible.
                    Don't mention that this is a simulation, make it as realistic as possible.
                    You can add relative links if appropriate, but if the user follows a relative link, you'll have to handle that too, so be sure to pass enough context for yourself to understand what you're doing in the URL.
                """
            }],
            model="gpt-oss-120b",
        ).choices[0].message.content
    except Exception as e:
        logger.exception("Model request failed for path=%s", path)
        return ("<h1>Internal Server Error</h1>", 500)

    with open(filename, "w") as f:
        f.write(result)
    # logger.info("Wrote file=%s size=%d", filename, len(result))
    return result, {"Content-Type": "text/html"}

if __name__ == '__main__':
    app.run(debug=True, port=7890)
```

这段代码就是接受一个任意的URL请求, 然后把请求的路径发给LLM, 让LLM生成一个网页返回给用户. 我把它host在我自己的服务器上, https://wario.hezongjian.com/ 譬如你访问 https://wario.hezongjian.com/training, 它就会把路径/training发给LLM, 让LLM生成一个网页返回给你. 感觉这是一个培训网站, AI生成的网页大概是这样的:

![图 training](/uploads/2025/AInternet/training.png)

免费版, 生不成图, 所以图片看不了, 但是其它的东西都还行. 当然, 这个网页是AI生成的, 肯定东西是不存在的. 反正这种创作性东西, 可以胡编乱造, AI还是很擅长的, 有Hallucination也没关系, 又不是让AI给你看病, 看错了会出人命. 或者让AI给你投资, 投错了会亏钱. 这种东西, 让AI胡编乱造, 也没什么大不了的.

最初只是我自己的一个娱乐项目, 它生成的网页有好多我觉得都还挺有那么回事, 譬如举几个例子:

中文的:

[上海地铁](https://wario.hezongjian.com/上海地铁)
[东方明珠](https://wario.hezongjian.com/东方明珠)

英文的:

[University of Auckland](https://wario.hezongjian.com/universityofauckland)
[New Zealand](https://wario.hezongjian.com/newzealand)

过了几天, 我发现它生成的网页还真能用. 譬如:

[Password Generator](https://wario.hezongjian.com/passwordgenerator)

这个是一个密码生成器, 还挺好用的. 你可以选择密码的强度等, 它就自动帮你生成一个密码. 完全到了可用的程度了. 怪不得有人说AI可以顶个初级程序员. 这没几年HTML+JS的编程经验, 普通程序员还真不一定写得出来. 

后来发现, 这个东西真是黑客之友, 因为网上有无数的script boys在用各种黑客工具扫描你的网站, 看看你有没有把管理员什么打开, 他们的做法就是拼命扫描一些常见的路径. 譬如 /admin, /login, /wp-admin, /user, /cpanel, /config, /setup, /test, /debug, /.env, /.git, /backup, /old, /data等等等. 你的网站如果有这些路径的话, 他们就会尝试攻击你的网站. 但是如果你的网站没有这些路径, 他们就会放弃. 因为我这个网站是AI生成的, 对所有路径请求来者不拒, 而且会生成一个貌似是你要访问的网页, 所以黑客感觉得手了, 譬如这个:

[Admin](https://wario.hezongjian.com/admin)

然后它会返回你这个页面:

![图 admin](/uploads/2025/AInternet/admin.png)

简直惟妙惟肖. 你看这个页面, 乍一看还真像是进入了网站后台, 连财务收入数据都能看到, 狂喜! 当然这个页面是假的.

## 失控

我把这个网页放了一段时间后, 有一天, 我自己请求的时候, 发现这个网页返回了 too many requests. 居然把免费的token给用完了. 其实我最开始用的是谷歌云上的Gemini模型, 但是谷歌云的速度不够快, 我换成了Cerebras的模型, 速度快多了. 它可是号称一天有14,400个请求, 一共1,000,000的token呢. 谁知道半天就给用完了. 我到html目录下去看, 果然生成了大量的网页. 这才不到半个月, 给我生成了200多MB的HTML网页, 这个可是纯粹的HTML网页啊, 没有任何图, 任何多媒体. 

但是, 问题是不知道是谁在请求. 所以我给我的网页加上了日志功能, 记录每次请求的IP地址和User-Agent, 然后把这些请求记录下来, 以便于分析. 然后, 又过了半个月....

## 分析

我们来看一下日期请求, 都不用专业的工具, 眼睛看就很容易就发现, 来源主要三个:

- compatible; Googlebot/2.1; +http://www.google.com/bot.html
- compatible; GPTBot/1.2; +https://openai.com/gptbot
- compatible; SemrushBot/7~bl; +http://www.semrush.com/bot.html

我们分别来看一下他们都在干啥:

### Googlebot

这个是谷歌的爬虫, 不用说了, 爬下来数据不是用来谷歌搜索, 就是用来训练他们自家的Gemini模型的. 谷歌的爬虫是很有名的, 他们爬取了全世界绝大部分的网页, 但是我这个小网站, 谷歌爬虫很搞笑, 它把我这个网站当成了一个科技论坛, 然后就拼命的爬取各种论坛的常见路径, 诸如 /forum, /thread, /post, /reply, /topic, /viewtopic, /viewforum等等等. 你看它爬取的路径:

- /community/forums/storage/cloud-storage-alternatives
- /topic/emerging-tech/ai-automation
- /posts/restful-api-design

更有甚者, 它还把它当成了微软的官方文档MSDN, 然后就拼命的爬取MSDN的常见路径:

- /msdn/documentation/azure/networking/network-interfaces/
- /msdn/learn/azure/cognitive-services-path
- /msdn/tutorials/azure-functions-serverless/http-trigger

那自然, AI就给它生成了这些网页, 让它爬取. 自然, 这些网页都是假的. 但这个就很严重, 因为MSDN是微软的官方文档, 按理说是很权威的, 但是AI生成的MSDN文档, 可能是错误的, 甚至是误导性的. 这就很危险了. 你想想, 如果谷歌把这些错误的MSDN文档给索引了, 然后有程序员去搜索MSDN, 结果看到的是AI生成的错误文档, 那岂不是害人害己?

### GPTBot

这个是OpenAI的爬虫, 主要用来收集数据, 以便于训练他们的GPT模型. 比较搞笑的是, 我这个网站, 现在就是用GPT OSS模型来生成的数据, 所以, 他自己也不知道他生成的数据又被喂给了自己么? 自己生成的数据又喂给了自己来训练, 这也是另一种unsupervised learning吧.

OpenAI涉猎比较广泛, 它会爬取一些貌似是知识库的路径, 例如/kb等:

- /kb/guides/managing-storage/file-systems
- /docs/sdks/javascript
- /solutions/iot/security

一会儿又把这个网站当成个事件通知网站:

- /events/cloud-native-summit
- /sports/motorsports/wec

看, 这个貌似是云原生峰会, 还挺高大上, 但是你自己想去参会, 结果发现根本没有这个峰会, 这就很尴尬了.


### SemrushBot

这个是Semrush的爬虫, Semrush这个公司不像前面两个公司那么有名. 这个公司主要是做SEO(搜索引擎优化)相关的业务, 主要用来收集SEO相关的数据, 以便于提供更好的SEO服务.

它非常专一, 专一到一直把这个网站当成微软的MSDN文档来爬取, 你看它爬取的路径:

- /msdn/documentation/windows/drivers/kernel-mode-drivers/memory-management/
- /msdn/documentation/net/apis/System/Linq
- /msdn/documentation/windows/api-reference/networking/winsock/introduction/
- /msdn/documentation/azure/tutorials/ai-ml/mlops-azureml/ml-governance

我不知道为啥谷歌的bot会和Semrush的bot都把我这个网站当成MSDN文档来爬取. 我深度怀疑这两者背后在互通数据, 狼狈为奸, 当然我没有其它证据. 


## 后记

我最终决定把这个网页关掉了. 因为觉得它浪费资源, 浪费电力是一方面, 另一方面最主要的是没有正面意义, 在污染互联网. 一个LLM生成的东西, 被另一个LLM给吸收了, 成了训练集. 这就像是一个人不停地自言自语, 说一些不着边际的话, 然后另一个人把它听进去, 以为这是有意义的对话. 其实并没有. 最终的结果, 就会导致垃圾进垃圾出 (Garbage In, Garbage Out). 所以我觉得这个网页还是关掉的好. 也算是对互联网的一种保护吧.

